{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f3bfb1-7353-4888-8fdf-9451db6f0678",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import gradio as gr\n",
    "\n",
    "# Function to interact with Ollama API\n",
    "def ollama_llm(question):\n",
    "    url = \"http://localhost:11434/api/generate\"  # Adjust the URL if your Ollama server is hosted elsewhere\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    payload = {\n",
    "        \"model\": \"llama3.3\",\n",
    "        \"prompt\": f\"總是用繁體中文回答！\\n\\nQuestion: {question}\",\n",
    "        \"stream\": False  # Ensure the response is not streamed\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        start_time = time.time()\n",
    "        response = requests.post(url, json=payload, headers=headers)\n",
    "        end_time = time.time()\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            response_data = response.json()\n",
    "            generated_text = response_data.get(\"response\", \"\")\n",
    "\n",
    "\n",
    "            eval_count = response_data.get(\"eval_count\", False)\n",
    "            eval_duration = response_data.get(\"eval_duration\", False) / 1e9  # 將 ns 轉為秒\n",
    "            prompt_eval_count = response_data.get(\"prompt_eval_count\", False)\n",
    "            prompt_eval_duration = response_data.get(\"prompt_eval_duration\", False) / 1e9  # 將 ns 轉為秒\n",
    "            total_duration = response_data.get(\"total_duration\", False) / 1e9  # 將 ns 轉為秒\n",
    "\n",
    "            # 計算 token 數量和時間\n",
    "            total_tokens = eval_count + prompt_eval_count\n",
    "            tokens_per_second = total_tokens / total_duration if total_duration > 0 else 0\n",
    "            \n",
    "            elapsed_time = end_time - start_time\n",
    "                                    \n",
    "            return (\n",
    "                f\"回答內容：\\n{generated_text}\\n\\n\"\n",
    "                f\"完整執行時間：{elapsed_time:.2f} 秒\\n\"\n",
    "                f\"Token 使用量：\\n\"\n",
    "                f\"- Prompt Tokens: {prompt_eval_count}\\n\"\n",
    "                f\"- Completion Tokens: {eval_count}\\n\"\n",
    "                f\"- Total Tokens: {total_tokens}\\n\"\n",
    "                f\"- Total Duration: {total_duration:.6f} seconds\\n\"\n",
    "                f\"Tokens per Second: {tokens_per_second:.2f}\"\n",
    "            )\n",
    "        else:\n",
    "            return f\"An error occurred: {response.status_code} - {response.text}\"\n",
    "    except Exception as e:\n",
    "        return f\"An error occurred: {str(e)}\"\n",
    "\n",
    "# Define Gradio interface\n",
    "def get_important_facts(question):\n",
    "    return ollama_llm(question)\n",
    "\n",
    "# Create Gradio app\n",
    "iface = gr.Interface(\n",
    "    fn=get_important_facts,\n",
    "    inputs=gr.Textbox(lines=2, placeholder=\"請輸入您的問題\"),\n",
    "    outputs=\"text\",\n",
    "    title=\"Ollama Chat\",\n",
    "    description=\"使用 Llama3 模型直接回答您的問題，並顯示執行時間和 Token 使用量。\",\n",
    ")\n",
    "\n",
    "# Launch Gradio app\n",
    "iface.launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
